{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8260bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa5ca4-ede3-4aea-a947-36dc2a732d58",
   "metadata": {},
   "source": [
    "### GNN Batching\n",
    "\n",
    "To implement the batching based on the Graph neighbors we should build a sampler that wraps Geometric's NeighborSampler and saves an attribute with the other outputs each time the __iter function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd78293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborBatcher(Sampler[int]):\n",
    "    r\"\"\"Samples elements sequentially, always in the same order.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "    data_source: Sized\n",
    "\n",
    "    def __init__(self, sampler_args) -> None:\n",
    "        self.NeighborSampler = NeighborSampler(sampler_args)\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        # This should yield an iteration of the indexes produced by NeighborSampler and save the other attributes in self\n",
    "        \n",
    "        return iter(range(len(self.data_source)))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.NeighborSampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db172255",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29cb2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "\n",
    "# Helper Functions\n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().cpu())\n",
    "            max_grads.append(p.grad.abs().max().cpu())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e258be71-ce98-45bd-99fa-8bc98ec4c433",
   "metadata": {},
   "source": [
    "Load Tokenizer, Validation and Training Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea79c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT Tokenizer\n",
    "from transformers import RobertaTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
    "import torch, os\n",
    "from tokenizers.processors import RobertaProcessing, BertProcessing\n",
    "\n",
    "## Global Parameters\n",
    "MAX_SEQ_LEN, MAX_TW_LEN = 128, 15\n",
    "BATCH_SIZE = 64\n",
    "SEED = 1911\n",
    "INTERACTION_TYPES = ['<cls>', '<pad>', 'Original', 'Quote', 'Reply', 'Retweet']\n",
    "##\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokDir = r'\\RoBETO_Model\\roberta_es_tweet_tokenizer_bpe_50k' #Office Directory\n",
    "\n",
    "tokenizer = RobertaTokenizerFast(os.path.join(tokDir, 'vocab.json'), os.path.join(tokDir, 'merges.txt'), \n",
    "                                tokenizer_file = os.path.join(tokDir, 'es_tweet_tokenizer_bpe_50k.json'), max_len = MAX_SEQ_LEN)\n",
    "# I use this instead of Robertaprocessing as it returns different IDs for the target and reply (it does not follow the Roberta Convention <s>...<\\s><\\s>...<\\s> and uses BERT's  <s>...<\\s>...<\\s>)        \n",
    "tokenizer._tokenizer.post_processor = BertProcessing( \n",
    "                                            (tokenizer.eos_token, tokenizer.eos_token_id),\n",
    "                                            (tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "                                        )\n",
    "PAD_ID, CLS_ID, SEP_ID = tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4cb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from UserModules.StanceDataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "workDir = r'\\Data' # Debugging Sample\n",
    "country = 'Ecuador'\n",
    "\n",
    "# Define Training Dataset\n",
    "num_workers = 6\n",
    "stance_params = {\n",
    "    'user_file_name': os.path.join(workDir, r\"2a-train_{}_dataframe.csv\".format(country)),\n",
    "    'tokenizer': tokenizer,\n",
    "    'interaction_categories': INTERACTION_TYPES,\n",
    "    'max_tw_per_user': MAX_TW_LEN,\n",
    "    'label_column': 'user_government_stance',\n",
    "    'max_seq_len':MAX_SEQ_LEN\n",
    "}\n",
    "replacement = True\n",
    "trainConfig = StanceDatasetConfig(**stance_params)\n",
    "train_data = StanceDataset(trainConfig)\n",
    "\n",
    "# Define Validation Dataset\n",
    "stance_params['user_file_name'] = os.path.join(workDir, r\"2b-validation_{}_dataframe.csv\".format(country))\n",
    "valConfig = StanceDatasetConfig(**stance_params)\n",
    "val_data = StanceDataset(valConfig)\n",
    "\n",
    "train_loader =  DataLoader(train_data, batch_size = BATCH_SIZE, num_workers = num_workers, collate_fn = train_data._Stance_datacollator, \n",
    "                           sampler = train_data._Balanced_sampler(replacement = replacement)) # Do Balanced Samples due to label assymetry\n",
    "val_loader = DataLoader(val_data, batch_size = BATCH_SIZE, num_workers = num_workers, collate_fn = val_data._Stance_datacollator, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beda34a-1415-4ced-b7f4-fdd0ba015d28",
   "metadata": {},
   "source": [
    "Define the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f947b0b7-d250-4c2e-a9ec-bbf19a2d1c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at E:\\OneDrive\\Research Group\\Papers\\Sudaka_BETO\\Data\\ROP_Task\\RoBETO_Model were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from UserModules.ModelConfiguration import *\n",
    "from UserModules.UserClassifier import User_Stance_Classifier\n",
    "\n",
    "tweet_enc_args = {\n",
    "    'Model_Dir': r'\\RoBETO_Model', \n",
    "    #'Model_Dir': r'/disk1/target_stance_classification/Data/RoBETO', \n",
    "    'dropout': 0.1,\n",
    "    'activation': 'Tanh',\n",
    "    'freeze_bert_embeddings': True\n",
    "}\n",
    "\n",
    "tweetConfig = RoBERTaEncoderConfig(**tweet_enc_args)\n",
    "\n",
    "emb_params = {\n",
    "    'cls_idx': val_data.config.tweet_cls_id,\n",
    "    'pad_idx': val_data.config.tweet_pad_id,\n",
    "    'max_tweet_number': MAX_TW_LEN,\n",
    "    'dropout': 0.1,\n",
    "    'layer_norm_eps': 1e-12,\n",
    "    'tweet_type_number': len(val_data.config.interaction_categories),\n",
    "    'mask_embeddings': True\n",
    "}\n",
    "\n",
    "embConfig = ModelEmbeddingsConfig(tweetConfig, **emb_params)\n",
    "\n",
    "user_params = { # This are the default parameters in UserEncoderConfig\n",
    "    'num_attention_heads': 6,\n",
    "    'intermidiate_size': 2048,\n",
    "    'num_encoder_layers': 3,\n",
    "    'transformer_activation': 'gelu',\n",
    "    'user_activation': 'Tanh',\n",
    "    'dropout': 0.1, \n",
    "    'initializer_range': 0.02,\n",
    "    'model_embedder_version': 'v3' # v3 leaves the CLS parameter for the type embeddings    \n",
    "}\n",
    "userConfig = UserEncoderConfig(embConfig, **user_params)\n",
    "\n",
    "# Instantiate model\n",
    "model = User_Stance_Classifier(num_classes = 2, user_config = userConfig)    \n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9072d-1ae1-409e-a197-02b4aa475cb9",
   "metadata": {},
   "source": [
    "Define the Trainer and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1bd387-f838-4e3c-b82c-6eed3ca85d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ac6474dc1d4523ae981fe56c0f036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from UserModules.Trainer import Metric, TrainerConfig, Trainer\n",
    "\n",
    "##\n",
    "METRICS = [Metric('accuracy_score', {'normalize': True}), Metric('f1_score', {'average': 'weighted'})]\n",
    "EPOCHS = 4#20\n",
    "results_path = r'\\Checkpoints'\n",
    "##\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = int(1.5 * len(train_loader)),# len(train_loader), # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "EVALS_PER_EPOCH = 4#10\n",
    "config = TrainerConfig(\n",
    "                        epochs_to_train = EPOCHS,\n",
    "                        results_path = results_path,\n",
    "                        optimizer = optimizer, \n",
    "                        batch_size = BATCH_SIZE, \n",
    "                        train_dataloader = train_loader,\n",
    "                        val_dataloader = val_loader,\n",
    "                        device = device, \n",
    "                        loss = torch.nn.CrossEntropyLoss(),\n",
    "                        scheduler = scheduler,\n",
    "                        clip_max_norm = 1,\n",
    "                        steps_to_eval = len(train_loader) // EVALS_PER_EPOCH,\n",
    "                        early_stop_eval_steps = EVALS_PER_EPOCH * 2,\n",
    "                        max_checkpoints_num = 10,\n",
    "                        seed = SEED,\n",
    "                        use_notebook = True,\n",
    "                        x_names = ['input_ids', 'attention_mask', 'interaction_types', 'tweet_masks'],\n",
    "                        y_name = 'labels',\n",
    "                        resume_checkpoint_path = os.path.join(results_path, 'checkpoint_273'), #None,\n",
    "                        metrics2collect = METRICS,\n",
    "                        metric_to_focus = 'loss',\n",
    "                        lowerIsbetter = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9698473-a210-4836-ba4c-b6e3b9b98ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e2230212914655b2c61cda4ea73c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65164fa3642b4e4ebccb5efa0ac24cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d80d4e6f334fa2b242e6de7f75e25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60fd733a9ce4805bddbb56afb2217c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f6e3f70f4744fa9fc885239d57c939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_351\n",
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\Best_Models\\checkpoint_351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00713d0f3444965ab0ac5f7f3715996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc41384d761416f83e5319a394a9cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192914432f584358afbfd9f59536a286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_468\n",
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\Best_Models\\checkpoint_468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f638adf6413a4e42afdb5ae83fa74670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daf0598bfa0429e9a9985f2c5444344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b244cf8c23f490283ce0f6fe17abfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e36944fab404042b6d75adbfa5aea92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583dfe8a526c47f496a48f6c62325d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\checkpoint_624\n",
      "    Model saved to ==> E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Small_Test\\Checkpoints\\Best_Models\\checkpoint_624\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b00d6-88cd-44c5-ae56-3e7ac1b2748b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50884b2-c31b-4830-9a70-31717bc3b838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766bc50-9501-48f2-91cd-17e5369042a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "533bccc2-53b6-41b7-9420-bdc226e0f6cb",
   "metadata": {},
   "source": [
    "### Create Small sample for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff9eccb-2c2e-4452-b586-c53a21face5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Small example\n",
    "import pandas as pd, os\n",
    "\n",
    "workDir = r'' # Office\n",
    "country = 'Bolivia'\n",
    "outDir = r''\n",
    " \n",
    "# Get small subset of data    \n",
    "trainDF = pd.read_csv(os.path.join(workDir, r\"2a-train_{}_dataframe.csv\".format(country)), dtype = {'tweet_id': 'Int64', 'user_id': str})\n",
    "uGroup = trainDF.groupby('user_id')\n",
    "smallTrain = []\n",
    "for i, (uID, uDF) in enumerate(uGroup):\n",
    "    smallTrain.append(uDF)\n",
    "    if i == 1000:    break\n",
    "trainDF = pd.concat(smallTrain, ignore_index = True)    \n",
    "trainDF.to_csv(r'{}\\2a-train_{}_dataframe.csv'.format(outDir, country), index = False)\n",
    "\n",
    "valDF = pd.read_csv(os.path.join(workDir, r\"2b-validation_{}_dataframe.csv\".format(country)), dtype = {'tweet_id': 'Int64', 'user_id': str})\n",
    "uGroup = trainDF.groupby('user_id')\n",
    "smallVal = []\n",
    "for i, (uID, uDF) in enumerate(uGroup):\n",
    "    smallVal.append(uDF)\n",
    "    if i == 100:    break\n",
    "valDF = pd.concat(smallVal, ignore_index = True)    \n",
    "valDF.to_csv(r'{}\\2b-validation_{}_dataframe.csv'.format(outDir, country), index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fc3d34-72eb-438b-8b79-2104366b6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small sample of the unbalanced Ecuadorian Data\n",
    "\n",
    "import pandas as pd, os\n",
    "\n",
    "workDir = r'.\\' # Office\n",
    "country = 'Ecuador'\n",
    "outDir = r'.\\'\n",
    " \n",
    "# Get small subset of data    \n",
    "trainDF = pd.read_csv(os.path.join(workDir, r\"2a-train_{}_dataframe.csv\".format(country)), dtype = {'tweet_id': 'Int64', 'user_id': str})\n",
    "uDF = trainDF.groupby('user_id').tweet_id.count().reset_index().rename(columns = {'tweet_id': 'counts'})\n",
    "uDF = uDF.sample(frac = 1)\n",
    "uDF = uDF.head(10000)\n",
    "\n",
    "trainDF = pd.merge(trainDF, uDF, on = 'user_id').drop(columns = 'counts')\n",
    "trainDF.to_csv(r'{}\\2a-train_{}_dataframe.csv'.format(outDir, country), index = False)\n",
    "\n",
    "valDF = pd.read_csv(os.path.join(workDir, r\"2b-validation_{}_dataframe.csv\".format(country)), dtype = {'tweet_id': 'Int64', 'user_id': str})\n",
    "uDF = valDF.groupby('user_id').tweet_id.count().reset_index().rename(columns = {'tweet_id': 'counts'})\n",
    "uDF = uDF.sample(frac = 1)\n",
    "uDF = uDF.head(1000)\n",
    "\n",
    "valDF = pd.merge(valDF, uDF, on = 'user_id').drop(columns = 'counts')\n",
    "valDF.to_csv(r'{}\\2b-validation_{}_dataframe.csv'.format(outDir, country), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b78197-e2c6-41c7-897e-9985b0e20336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "workDir = r'' # Office\n",
    "country = 'Ecuador'\n",
    "\n",
    "trainDF = pd.read_csv(os.path.join(workDir, r\"2a-train_{}_dataframe.csv\".format(country)), dtype = {'tweet_id': 'Int64', 'user_id': str})\n",
    "\n",
    "userDF = trainDF.groupby('user_id').user_government_stance.first().reset_index()\n",
    "stanceDF = userDF.groupby('user_government_stance').count().reset_index().rename(columns = {'user_id': 'counts'})\n",
    "stanceDF['weights'] = len(userDF) / stanceDF['counts']\n",
    "userDF = pd.merge(userDF, stanceDF[['user_government_stance', 'weights']], on = 'user_government_stance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695f5451-b24c-4d70-87f2-8d1b4a700efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT Tokenizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import RobertaModel, RobertaTokenizerFast, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\n",
    "import torch, numpy as np, random, os\n",
    "from tokenizers.processors import RobertaProcessing, BertProcessing\n",
    "\n",
    "## Global Parameters\n",
    "MAX_SEQ_LEN, MAX_TW_LEN = 128, 15\n",
    "BATCH_SIZE = 32#64\n",
    "SEED = 1911\n",
    "INTERACTION_TYPES = ['<cls>', '<pad>', 'Original', 'Quote', 'Reply', 'Retweet']\n",
    "##\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#tokDir = r'/disk1/target_stance_classification/Data/RoBETO/roberta_es_tweet_tokenizer_bpe_50k' #Server Directory\n",
    "tokDir = r'\\RoBETO_Model\\roberta_es_tweet_tokenizer_bpe_50k' #Office Directory\n",
    "\n",
    "tokenizer = RobertaTokenizerFast(os.path.join(tokDir, 'vocab.json'), os.path.join(tokDir, 'merges.txt'), \n",
    "                                tokenizer_file = os.path.join(tokDir, 'es_tweet_tokenizer_bpe_50k.json'), max_len = MAX_SEQ_LEN)\n",
    "# I use this instead of Robertaprocessing as it returns different IDs for the target and reply (it does not follow the Roberta Convention <s>...<\\s><\\s>...<\\s> and uses BERT's  <s>...<\\s>...<\\s>)        \n",
    "tokenizer._tokenizer.post_processor = BertProcessing( \n",
    "                                            (tokenizer.eos_token, tokenizer.eos_token_id),\n",
    "                                            (tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "                                        )\n",
    "PAD_ID, CLS_ID, SEP_ID = tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from UserModules.StanceDataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#workDir = r'/disk1/target_stance_classification/Data/Splits/Subsampled' # Server\n",
    "#workDir = r'E:\\OneDrive\\Research Group\\Papers\\Target_Stance_Classification\\Data\\Splits\\Subsampled' # Office\n",
    "workDir = r'.\\' # Debugging Sample\n",
    "country = 'Ecuador'\n",
    "\n",
    "# Define Training Dataset\n",
    "num_workers = 6\n",
    "stance_params = {\n",
    "    'user_file_name': os.path.join(workDir, r\"2a-train_{}_dataframe.csv\".format(country)),\n",
    "    'tokenizer': tokenizer,\n",
    "    'interaction_categories': INTERACTION_TYPES,\n",
    "    'max_tw_per_user': MAX_TW_LEN,\n",
    "    'label_column': 'user_government_stance',\n",
    "    'max_seq_len':MAX_SEQ_LEN\n",
    "}\n",
    "trainConfig = StanceDatasetConfig(**stance_params)\n",
    "train_data = StanceDataset(trainConfig)\n",
    "\n",
    "# Define Validation Dataset\n",
    "stance_params['user_file_name'] = os.path.join(workDir, r\"2b-validation_{}_dataframe.csv\".format(country))\n",
    "valConfig = StanceDatasetConfig(**stance_params)\n",
    "val_data = StanceDataset(valConfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "781629f3-9465-41d1-ab99-b703e9748156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fDF = []\n",
    "replacement = True\n",
    "sampler = train_data._Balanced_sampler(replacement = replacement, num_samples = len(train_data))\n",
    "for i in sampler:\n",
    "    user_id = train_data.users[i]\n",
    "    uDF = train_data.User_DF.loc[train_data.User_DF.user_id == user_id]\n",
    "    target = uDF.user_stance.head(1).item()\n",
    "    fDF.append({'user_id': user_id, 'user_stance': target})\n",
    "sampledDF = pd.DataFrame(fDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "394f5724-640b-4d1e-b464-c95ab28c9ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_stance</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id\n",
       "user_stance         \n",
       "0               1513\n",
       "1               2440"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledDF[sampledDF.duplicated(subset = 'user_id')].groupby('user_stance').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b3b8917-d50b-4b89-b6e8-fbe19a4b80d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_stance</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id\n",
       "user_stance         \n",
       "0               4073\n",
       "1               1879"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledDF2.groupby('user_stance').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c86f97b2-10bb-4e17-b849-53afbe341158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_stance</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id\n",
       "user_stance         \n",
       "0               5066\n",
       "1               4934"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledDF.groupby('user_stance').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167f694-2b61-43f0-bcaf-7d78efc5cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader =  DataLoader(train_data, batch_size = BATCH_SIZE, num_workers = num_workers, collate_fn = train_data._Stance_datacollator, shuffle = True)\n",
    "val_loader = DataLoader(val_data, batch_size = BATCH_SIZE, num_workers = num_workers, collate_fn = val_data._Stance_datacollator, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b3ad319-0581-4439-8366-da743912ab44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_government_stance</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000048856523034624</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000145648</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100017027</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000217620027764738</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000356578</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>998062725220700160</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>999123320615456773</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>999255230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>999839718312480768</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>999995148376461312</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id  user_government_stance   weights\n",
       "0     1000048856523034624                       0  0.000150\n",
       "1              1000145648                       0  0.000150\n",
       "2               100017027                       0  0.000150\n",
       "3     1000217620027764738                       0  0.000150\n",
       "4              1000356578                       0  0.000150\n",
       "...                   ...                     ...       ...\n",
       "9995   998062725220700160                       1  0.000302\n",
       "9996   999123320615456773                       1  0.000302\n",
       "9997            999255230                       1  0.000302\n",
       "9998   999839718312480768                       1  0.000302\n",
       "9999   999995148376461312                       1  0.000302\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "https://github.com/ufoym/imbalanced-dataset-sampler/blob/master/torchsampler/imbalanced.py\n",
    "\n",
    "https://pytorch.org/docs/master/_modules/torch/utils/data/sampler.html#Sampler\n",
    "https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/28?page=2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e65c4ad9cc556b45f6f01fa77834d70af7d00dff5a48cc53bd96eb904ee2366"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
